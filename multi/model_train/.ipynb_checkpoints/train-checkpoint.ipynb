{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM 多分类模型训练\n",
    "\n",
    "用于抗性基因(ARG)类别分类\n",
    "\n",
    "**关键改进**:\n",
    "1. Focal Loss 处理类别不平衡\n",
    "2. 分层采样确保每个batch包含各类样本\n",
    "3. 自动合并稀有类别\n",
    "4. 保存配置信息到模型文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 配置参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ================== 模型配置（预测时必须保持一致）==================\n",
    "MODEL_CONFIG = {\n",
    "    'embedding_size': 21,    # One-hot编码维度 (20氨基酸 + PAD)\n",
    "    'hidden_size': 128,      # LSTM隐藏层维度\n",
    "    'num_layers': 2,         # LSTM层数\n",
    "    'dropout': 0.4,          # 降低Dropout，防止欠拟合\n",
    "}\n",
    "\n",
    "# ================== 训练配置 ==================\n",
    "TRAIN_CONFIG = {\n",
    "    'batch_size': 256,       # 增大batch size，让loss更平滑\n",
    "    'lr': 0.002,             # 稍微提高初始学习率\n",
    "    'warmup_epochs': 5,      # 学习率预热轮数\n",
    "    'epochs': 150,\n",
    "    'patience': 25,          # 增加早停耐心值\n",
    "    'min_samples': 50,       # 少于此数的类别合并为Others\n",
    "    'focal_gamma': 0.5,      # 降低gamma，减少对困难样本的过度关注\n",
    "    'label_smoothing': 0.1,  # 标签平滑，让训练更稳定\n",
    "}\n",
    "\n",
    "# ================== 路径配置（请修改为实际路径）==================\n",
    "PATH_CONFIG = {\n",
    "    # 说明：notebook 的相对路径会受当前工作目录影响（在根目录执行/在 notebook 目录执行会不同）。\n",
    "    # 这里做一个“向上查找 data/ 目录”的自动解析，减少路径踩坑。\n",
    "    'fasta_file': \"ARG_db_all_seq_uniq_representative_rename_2_repsent.fasta\",  # ARG 序列文件名（实际路径会自动解析到 repo_root/data/ 下）\n",
    "    'save_dir': \"./well-trained\",\n",
    "    'log_dir': \"./logs\",\n",
    "    'fig_dir': \"./figures\",\n",
    "}\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_repo_data_file(filename: str) -> str:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        cand = p / 'data' / filename\n",
    "        if cand.is_file():\n",
    "            return str(cand)\n",
    "    # fallback：按当前工作目录的相对路径尝试一次\n",
    "    cand = (cwd / filename)\n",
    "    if cand.is_file():\n",
    "        return str(cand)\n",
    "    raise FileNotFoundError(f\"无法找到数据文件：{filename}（已从 {cwd} 向上查找 data/ 目录）\")\n",
    "\n",
    "PATH_CONFIG['fasta_file'] = resolve_repo_data_file(PATH_CONFIG['fasta_file'])\n",
    "assert os.path.exists(PATH_CONFIG['fasta_file']), (\n",
    "    f\"FASTA 文件不存在：{PATH_CONFIG['fasta_file']}（请检查工作目录或修改 PATH_CONFIG）\"\n",
    ")\n",
    "\n",
    "# 设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 随机种子\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 创建目录\n",
    "for d in [PATH_CONFIG['save_dir'], PATH_CONFIG['log_dir'], PATH_CONFIG['fig_dir']]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# 日志\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 氨基酸编码字典\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "AA_DICT = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n",
    "AA_DICT.update({\n",
    "    'B': [AA_DICT['D'], AA_DICT['N']],  # Asp or Asn\n",
    "    'Z': [AA_DICT['E'], AA_DICT['Q']],  # Glu or Gln\n",
    "    'J': [AA_DICT['I'], AA_DICT['L']],  # Ile or Leu\n",
    "    'X': 'ANY',\n",
    "    'PAD': 20\n",
    "})\n",
    "\n",
    "def one_hot_encode(sequence, max_length):\n",
    "    \"\"\"将氨基酸序列转换为one-hot编码\"\"\"\n",
    "    encoding = np.zeros((max_length, 21), dtype=np.float32)\n",
    "    for i in range(min(len(sequence), max_length)):\n",
    "        aa = sequence[i]\n",
    "        if aa in AA_DICT:\n",
    "            idx = AA_DICT[aa]\n",
    "            if isinstance(idx, list):  # 模糊氨基酸\n",
    "                for j in idx:\n",
    "                    encoding[i, j] = 0.5\n",
    "            elif idx == 'ANY':  # 未知氨基酸\n",
    "                encoding[i, :20] = 0.05\n",
    "            else:\n",
    "                encoding[i, idx] = 1.0\n",
    "        else:\n",
    "            encoding[i, :20] = 0.05  # 未知字符\n",
    "    # 填充位置标记为PAD\n",
    "    if len(sequence) < max_length:\n",
    "        encoding[len(sequence):, 20] = 1.0\n",
    "    return encoding\n",
    "\n",
    "\n",
    "class ARGDataset(Dataset):\n",
    "    \"\"\"ARG序列数据集\"\"\"\n",
    "    def __init__(self, sequences, labels, max_length):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = one_hot_encode(self.sequences[idx], self.max_length)\n",
    "        return torch.from_numpy(encoded), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型定义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss with Label Smoothing for imbalanced classification\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 类别权重\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(\n",
    "            inputs, targets, weight=self.alpha, \n",
    "            reduction='none', label_smoothing=self.label_smoothing\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    \"\"\"BiLSTM + Global Pooling 多分类模型\"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config['embedding_size'],\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_layers=config['num_layers'],\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=config['dropout'] if config['num_layers'] > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        \n",
    "        # 双向LSTM输出 * 2 (max + avg pooling) = hidden_size * 4\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config['hidden_size'] * 4, config['hidden_size']),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(config['hidden_size'], num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)  # (batch, seq_len, hidden*2)\n",
    "        \n",
    "        # Masked Global Pooling（避免 PAD 位置污染 max/mean pooling）\n",
    "        # x 的最后一维为 one-hot(21)，其中 index=20 表示 PAD（one_hot_encode 中 padding 位置会置 1）\n",
    "        pad_flag = x[:, :, 20]  # (batch, seq_len)\n",
    "        valid_mask = pad_flag < 0.5  # True 表示真实氨基酸位置\n",
    "        mask = valid_mask.unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "        \n",
    "        # max pooling：把 PAD 位置设为极小值\n",
    "        output_masked = output.masked_fill(~mask, -1e9)\n",
    "        max_pool, _ = torch.max(output_masked, dim=1)\n",
    "        \n",
    "        # mean pooling：只对真实位置求平均\n",
    "        mask_f = mask.float()\n",
    "        sum_pool = (output * mask_f).sum(dim=1)\n",
    "        denom = mask_f.sum(dim=1).clamp(min=1.0)\n",
    "        avg_pool = sum_pool / denom\n",
    "        features = torch.cat([max_pool, avg_pool], dim=1)\n",
    "        \n",
    "        return self.classifier(self.dropout(features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加载数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 13:16:04,938 - Loading data from /home/mayue/model_c/data/ARG_db_all_seq_uniq_representative_rename_2_repsent.fasta\n",
      "2026-02-05 13:16:05,120 - Loaded 17345 sequences (skipped 0 due to header parse)\n",
      "2026-02-05 13:16:05,122 - Original classes: 30\n",
      "2026-02-05 13:16:05,122 - Merging 17 rare classes (< 50 samples) into 'Others'\n",
      "2026-02-05 13:16:05,132 - Max length (95th percentile): 657\n",
      "2026-02-05 13:16:05,132 - Final classes: 14\n",
      "2026-02-05 13:16:05,133 -   MLS: 1814\n",
      "2026-02-05 13:16:05,133 -   aminocoumarin: 98\n",
      "2026-02-05 13:16:05,133 -   aminoglycoside: 1067\n",
      "2026-02-05 13:16:05,134 -   beta-lactam: 7336\n",
      "2026-02-05 13:16:05,134 -   diaminopyrimidine: 238\n",
      "2026-02-05 13:16:05,134 -   glycopeptide: 437\n",
      "2026-02-05 13:16:05,134 -   multidrug: 3548\n",
      "2026-02-05 13:16:05,135 -   peptide: 757\n",
      "2026-02-05 13:16:05,135 -   phenicol: 222\n",
      "2026-02-05 13:16:05,135 -   phosphonic: 507\n",
      "2026-02-05 13:16:05,136 -   quinolone: 420\n",
      "2026-02-05 13:16:05,136 -   sulfonamide: 143\n",
      "2026-02-05 13:16:05,136 -   tetracycline: 530\n",
      "2026-02-05 13:16:05,137 -   Others: 228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num classes: 14\n",
      "Max length: 657\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(fasta_file, min_samples):\n",
    "    \"\"\"加载FASTA文件并预处理数据\"\"\"\n",
    "    logger.info(f\"Loading data from {fasta_file}\")\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    skipped = 0\n",
    "    \n",
    "    # 解析FASTA文件\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        try:\n",
    "            parts = record.description.split('|')\n",
    "            # 统一取最后一个 '|' 字段作为类别（兼容 MegaRes / SARG 等不同 header 风格）\n",
    "            if len(parts) >= 2:\n",
    "                raw_label = parts[-1].strip()\n",
    "                label = raw_label.split()[0] if raw_label else \"\"\n",
    "                if label:\n",
    "                    sequences.append(str(record.seq).upper())\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    skipped += 1\n",
    "            else:\n",
    "                skipped += 1\n",
    "        except Exception:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Loaded {len(sequences)} sequences (skipped {skipped} due to header parse)\")\n",
    "    \n",
    "    # 统计类别分布\n",
    "    label_counts = Counter(labels)\n",
    "    logger.info(f\"Original classes: {len(label_counts)}\")\n",
    "    \n",
    "    # 合并稀有类别\n",
    "    rare_classes = [c for c, count in label_counts.items() if count < min_samples]\n",
    "    if rare_classes:\n",
    "        logger.info(f\"Merging {len(rare_classes)} rare classes (< {min_samples} samples) into 'Others'\")\n",
    "        labels = [l if l not in rare_classes else 'Others' for l in labels]\n",
    "    \n",
    "    # 创建标签映射\n",
    "    unique_labels = sorted(set(labels))\n",
    "    if 'Others' in unique_labels:\n",
    "        unique_labels.remove('Others')\n",
    "        unique_labels.append('Others')  # Others放最后\n",
    "    \n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    \n",
    "    # 转换为数字标签\n",
    "    y = np.array([label_to_idx[l] for l in labels])\n",
    "    \n",
    "    # 计算max_length (95分位数)\n",
    "    lengths = [len(s) for s in sequences]\n",
    "    max_length = int(np.percentile(lengths, 95))\n",
    "    logger.info(f\"Max length (95th percentile): {max_length}\")\n",
    "    \n",
    "    # 类别分布\n",
    "    class_counts = np.bincount(y)\n",
    "    logger.info(f\"Final classes: {len(unique_labels)}\")\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        logger.info(f\"  {label}: {class_counts[i]}\")\n",
    "    \n",
    "    return sequences, y, unique_labels, label_to_idx, idx_to_label, max_length, class_counts\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "sequences, y, class_names, label_to_idx, idx_to_label, max_length, class_counts = \\\n",
    "    load_and_preprocess_data(PATH_CONFIG['fasta_file'], TRAIN_CONFIG['min_samples'])\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(f\"\\nNum classes: {num_classes}\")\n",
    "print(f\"Max length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 13:16:05,150 - Train: 13876, Val: 3469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 55, Val batches: 14\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集/验证集 (分层采样)\n",
    "train_seqs, val_seqs, y_train, y_val = train_test_split(\n",
    "    sequences, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "logger.info(f\"Train: {len(train_seqs)}, Val: {len(val_seqs)}\")\n",
    "\n",
    "# 创建Dataset\n",
    "train_dataset = ARGDataset(train_seqs, y_train, max_length)\n",
    "val_dataset = ARGDataset(val_seqs, y_val, max_length)\n",
    "\n",
    "# 创建DataLoader（移除分层采样，让Focal Loss单独处理类别不平衡）\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=TRAIN_CONFIG['batch_size'],\n",
    "    shuffle=True,  # 使用普通shuffle\n",
    "    num_workers=4, \n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=TRAIN_CONFIG['batch_size'],\n",
    "    shuffle=False, \n",
    "    num_workers=4, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_warmup_scheduler(optimizer, warmup_epochs, total_epochs, steps_per_epoch):\n",
    "    \"\"\"创建带有warmup的学习率调度器\"\"\"\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    total_steps = total_epochs * steps_per_epoch\n",
    "    \n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            # 线性warmup\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        else:\n",
    "            # 余弦退火\n",
    "            progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            return 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # === 1. 构建模型 ===\n",
    "    model = BiLSTMClassifier(MODEL_CONFIG, num_classes).to(device)\n",
    "    \n",
    "    # 计算类别权重（更温和的权重）\n",
    "    class_weights = torch.tensor(\n",
    "        np.sqrt(np.median(class_counts) / (class_counts + 1)),  # 使用平方根，减少极端权重\n",
    "        dtype=torch.float32\n",
    "    ).to(device)\n",
    "    class_weights = torch.clamp(class_weights, 0.5, 3.0)  # 限制权重范围\n",
    "    logger.info(f\"Class weights range: [{class_weights.min():.2f}, {class_weights.max():.2f}]\")\n",
    "    \n",
    "    # Focal Loss with Label Smoothing\n",
    "    criterion = FocalLoss(\n",
    "        alpha=class_weights, \n",
    "        gamma=TRAIN_CONFIG['focal_gamma'],\n",
    "        label_smoothing=TRAIN_CONFIG['label_smoothing']\n",
    "    )\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=TRAIN_CONFIG['lr'], weight_decay=1e-4)\n",
    "    \n",
    "    # 使用Warmup + Cosine退火调度器\n",
    "    scheduler = get_warmup_scheduler(\n",
    "        optimizer, \n",
    "        TRAIN_CONFIG['warmup_epochs'], \n",
    "        TRAIN_CONFIG['epochs'],\n",
    "        len(train_loader)\n",
    "    )\n",
    "    \n",
    "    # === 2. 训练循环 ===\n",
    "    best_f1_macro = -1.0\n",
    "    patience_cnt = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1_macro': [], 'lr': []}\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    save_path = os.path.join(PATH_CONFIG['save_dir'], f\"bilstm_multi_{timestamp}.pth\")\n",
    "    \n",
    "    logger.info(f\"Start training for {TRAIN_CONFIG['epochs']} epochs...\")\n",
    "    \n",
    "    for epoch in range(TRAIN_CONFIG['epochs']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        \n",
    "        for x, y_batch in train_loader:\n",
    "            x, y_batch = x.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # 每个batch更新学习率\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += (logits.argmax(dim=1) == y_batch).sum().item()\n",
    "            train_total += y_batch.size(0)\n",
    "        \n",
    "        # --- Validate ---\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        y_true_all, y_pred_all = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y_batch in val_loader:\n",
    "                x, y_batch = x.to(device), y_batch.to(device)\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += y_batch.size(0)\n",
    "                y_true_all.append(y_batch.detach().cpu())\n",
    "                y_pred_all.append(preds.detach().cpu())\n",
    "        \n",
    "        # --- Metrics ---\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        y_true_np = torch.cat(y_true_all).numpy() if y_true_all else np.array([])\n",
    "        y_pred_np = torch.cat(y_pred_all).numpy() if y_pred_all else np.array([])\n",
    "        val_f1_macro = f1_score(y_true_np, y_pred_np, average='macro') if y_true_np.size else 0.0\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1_macro'].append(val_f1_macro)\n",
    "        history['lr'].append(current_lr)\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch+1:03d} | Loss: {avg_train_loss:.4f}/{avg_val_loss:.4f} | \"\n",
    "            f\"Acc: {train_acc:.4f}/{val_acc:.4f} | Macro-F1: {val_f1_macro:.4f} | LR: {current_lr:.6f} | Time: {time.time()-start_time:.1f}s\"\n",
    "        )\n",
    "        \n",
    "        # --- Early Stopping ---\n",
    "        if val_f1_macro > best_f1_macro:\n",
    "            best_f1_macro = val_f1_macro\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_config': MODEL_CONFIG,\n",
    "                'class_names': class_names,\n",
    "                'label_to_idx': label_to_idx,\n",
    "                'max_length': max_length,\n",
    "                'best_val_f1_macro': best_f1_macro,\n",
    "            }, save_path)\n",
    "            patience_cnt = 0\n",
    "            logger.info(f\"  -> Best model saved! (Macro-F1: {best_f1_macro:.4f}, Acc: {val_acc:.4f})\")\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= TRAIN_CONFIG['patience']:\n",
    "                logger.info(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # === 3. 评估 ===\n",
    "    evaluate_and_plot(save_path, val_loader, history)\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 评估与可视化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(model_path, dataloader, history):\n",
    "    \"\"\"加载最佳模型进行评估并绘制图表\"\"\"\n",
    "    logger.info(\"Running final evaluation...\")\n",
    "    \n",
    "    # 加载模型\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model = BiLSTMClassifier(checkpoint['model_config'], len(checkpoint['class_names'])).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # 预测\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y_batch in dataloader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            y_true.extend(y_batch.numpy())\n",
    "            y_pred.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # 打印分类报告\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(classification_report(y_true, y_pred, target_names=checkpoint['class_names']))\n",
    "    print(f\"Overall Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Macro-F1: {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 绘图 (2x2布局)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Loss曲线\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss Curve')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy / Macro-F1 曲线\n",
    "    axes[0, 1].plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "    axes[0, 1].plot(history['val_acc'], label='Val Acc', linewidth=2)\n",
    "    if 'val_f1_macro' in history:\n",
    "        axes[0, 1].plot(history['val_f1_macro'], label='Val Macro-F1', linewidth=2, linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].set_title('Accuracy / Macro-F1 Curve')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 学习率曲线\n",
    "    axes[1, 0].plot(history['lr'], color='green', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule (Warmup + Cosine)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 混淆矩阵\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=checkpoint['class_names'],\n",
    "                yticklabels=checkpoint['class_names'], ax=axes[1, 1])\n",
    "    axes[1, 1].set_xlabel('Predicted')\n",
    "    axes[1, 1].set_ylabel('True')\n",
    "    axes[1, 1].set_title('Confusion Matrix')\n",
    "    plt.setp(axes[1, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(axes[1, 1].get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PATH_CONFIG['fig_dir'], 'training_results.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(PATH_CONFIG['fig_dir'], 'training_results.pdf'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"Figures saved to {PATH_CONFIG['fig_dir']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 开始训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 13:16:06,592 - Class weights range: [0.50, 2.18]\n",
      "2026-02-05 13:16:16,789 - Start training for 150 epochs...\n",
      "2026-02-05 13:16:24,894 - Epoch 001 | Loss: 1.5657/1.4465 | Acc: 0.2807/0.4229 | Macro-F1: 0.0425 | LR: 0.000400 | Time: 8.1s\n",
      "2026-02-05 13:16:24,989 -   -> Best model saved! (Macro-F1: 0.0425, Acc: 0.4229)\n",
      "2026-02-05 13:16:30,762 - Epoch 002 | Loss: 1.4476/1.4098 | Acc: 0.4227/0.4229 | Macro-F1: 0.0425 | LR: 0.000800 | Time: 5.8s\n",
      "2026-02-05 13:16:36,521 - Epoch 003 | Loss: 1.3869/1.2690 | Acc: 0.4273/0.4503 | Macro-F1: 0.0813 | LR: 0.001200 | Time: 5.8s\n",
      "2026-02-05 13:16:36,629 -   -> Best model saved! (Macro-F1: 0.0813, Acc: 0.4503)\n",
      "2026-02-05 13:16:42,373 - Epoch 004 | Loss: 1.2627/1.1734 | Acc: 0.4822/0.5293 | Macro-F1: 0.1757 | LR: 0.001600 | Time: 5.7s\n",
      "2026-02-05 13:16:42,386 -   -> Best model saved! (Macro-F1: 0.1757, Acc: 0.5293)\n",
      "2026-02-05 13:16:48,185 - Epoch 005 | Loss: 1.2446/1.2260 | Acc: 0.4979/0.5137 | Macro-F1: 0.1893 | LR: 0.002000 | Time: 5.8s\n",
      "2026-02-05 13:16:48,198 -   -> Best model saved! (Macro-F1: 0.1893, Acc: 0.5137)\n",
      "2026-02-05 13:16:54,058 - Epoch 006 | Loss: 1.1524/1.0334 | Acc: 0.5631/0.6627 | Macro-F1: 0.3319 | LR: 0.002000 | Time: 5.9s\n",
      "2026-02-05 13:16:54,075 -   -> Best model saved! (Macro-F1: 0.3319, Acc: 0.6627)\n",
      "2026-02-05 13:16:59,957 - Epoch 007 | Loss: 1.0317/0.8957 | Acc: 0.6591/0.7639 | Macro-F1: 0.4016 | LR: 0.001999 | Time: 5.9s\n",
      "2026-02-05 13:16:59,986 -   -> Best model saved! (Macro-F1: 0.4016, Acc: 0.7639)\n",
      "2026-02-05 13:17:05,905 - Epoch 008 | Loss: 0.9143/0.7982 | Acc: 0.7574/0.8201 | Macro-F1: 0.5402 | LR: 0.001998 | Time: 5.9s\n",
      "2026-02-05 13:17:05,924 -   -> Best model saved! (Macro-F1: 0.5402, Acc: 0.8201)\n",
      "2026-02-05 13:17:11,816 - Epoch 009 | Loss: 0.8428/0.7445 | Acc: 0.7886/0.8374 | Macro-F1: 0.5767 | LR: 0.001996 | Time: 5.9s\n",
      "2026-02-05 13:17:11,829 -   -> Best model saved! (Macro-F1: 0.5767, Acc: 0.8374)\n",
      "2026-02-05 13:17:17,716 - Epoch 010 | Loss: 0.7890/0.6919 | Acc: 0.8230/0.8504 | Macro-F1: 0.6330 | LR: 0.001994 | Time: 5.9s\n",
      "2026-02-05 13:17:17,730 -   -> Best model saved! (Macro-F1: 0.6330, Acc: 0.8504)\n",
      "2026-02-05 13:17:23,645 - Epoch 011 | Loss: 0.7277/0.6785 | Acc: 0.8552/0.8628 | Macro-F1: 0.7147 | LR: 0.001992 | Time: 5.9s\n",
      "2026-02-05 13:17:23,659 -   -> Best model saved! (Macro-F1: 0.7147, Acc: 0.8628)\n",
      "2026-02-05 13:17:29,658 - Epoch 012 | Loss: 0.6776/0.6011 | Acc: 0.8792/0.9023 | Macro-F1: 0.7785 | LR: 0.001989 | Time: 6.0s\n",
      "2026-02-05 13:17:29,672 -   -> Best model saved! (Macro-F1: 0.7785, Acc: 0.9023)\n",
      "2026-02-05 13:17:35,634 - Epoch 013 | Loss: 0.6399/0.5891 | Acc: 0.8956/0.9072 | Macro-F1: 0.7984 | LR: 0.001985 | Time: 6.0s\n",
      "2026-02-05 13:17:35,648 -   -> Best model saved! (Macro-F1: 0.7984, Acc: 0.9072)\n",
      "2026-02-05 13:17:41,577 - Epoch 014 | Loss: 0.6156/0.5479 | Acc: 0.9075/0.9323 | Macro-F1: 0.8336 | LR: 0.001981 | Time: 5.9s\n",
      "2026-02-05 13:17:41,591 -   -> Best model saved! (Macro-F1: 0.8336, Acc: 0.9323)\n",
      "2026-02-05 13:17:47,586 - Epoch 015 | Loss: 0.5917/0.5263 | Acc: 0.9174/0.9320 | Macro-F1: 0.8336 | LR: 0.001977 | Time: 6.0s\n",
      "2026-02-05 13:17:47,599 -   -> Best model saved! (Macro-F1: 0.8336, Acc: 0.9320)\n",
      "2026-02-05 13:17:53,591 - Epoch 016 | Loss: 0.5685/0.5072 | Acc: 0.9263/0.9395 | Macro-F1: 0.8514 | LR: 0.001972 | Time: 6.0s\n",
      "2026-02-05 13:17:53,604 -   -> Best model saved! (Macro-F1: 0.8514, Acc: 0.9395)\n",
      "2026-02-05 13:17:59,567 - Epoch 017 | Loss: 0.5468/0.4937 | Acc: 0.9360/0.9467 | Macro-F1: 0.8858 | LR: 0.001966 | Time: 6.0s\n",
      "2026-02-05 13:17:59,581 -   -> Best model saved! (Macro-F1: 0.8858, Acc: 0.9467)\n"
     ]
    }
   ],
   "source": [
    "# 运行训练\n",
    "model_path = train()\n",
    "print(f\"\\nModel saved to: {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gene_pred)",
   "language": "python",
   "name": "gene_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
